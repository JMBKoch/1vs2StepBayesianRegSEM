
@article{van_erp_shrinkage_2019,
	title = {Shrinkage priors for {Bayesian} penalized regression},
	volume = {89},
	issn = {00222496},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249618300567},
	doi = {10.1016/j.jmp.2018.12.004},
	abstract = {In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.},
	language = {en},
	urldate = {2021-06-03},
	journal = {Journal of Mathematical Psychology},
	author = {Van Erp, Sara and Oberski, Daniel L. and Mulder, Joris},
	month = apr,
	year = {2019},
	pages = {31--50},
	file = {van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:/home/michi/Zotero/storage/4S7YATAQ/van Erp et al. - 2019 - Shrinkage priors for Bayesian penalized regression.pdf:application/pdf},
}

@article{jacobucci_regularized_2016,
	title = {Regularized {Structural} {Equation} {Modeling}},
	volume = {23},
	issn = {1070-5511, 1532-8007},
	url = {http://www.tandfonline.com/doi/full/10.1080/10705511.2016.1154793},
	doi = {10.1080/10705511.2016.1154793},
	abstract = {A new method is proposed that extends the use of regularization in both lasso and ridge regression to structural equation models. The method is termed regularized structural equation modeling (RegSEM). RegSEM penalizes speciﬁc parameters in structural equation models, with the goal of creating easier to understand and simpler models. Although regularization has gained wide adoption in regression, very little has transferred to models with latent variables. By adding penalties to speciﬁc parameters in a structural equation model, researchers have a high level of ﬂexibility in reducing model complexity, overcoming poor ﬁtting models, and the creation of models that are more likely to generalize to new samples. The proposed method was evaluated through a simulation study, two illustrative examples involving a measurement model, and one empirical example involving the structural part of the model to demonstrate RegSEM’s utility.},
	language = {en},
	number = {4},
	urldate = {2021-06-03},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Jacobucci, Ross and Grimm, Kevin J. and McArdle, John J.},
	month = jul,
	year = {2016},
	pages = {555--566},
	file = {Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:/home/michi/Zotero/storage/SFDUNLZN/Jacobucci et al. - 2016 - Regularized Structural Equation Modeling.pdf:application/pdf},
}

@article{muthen_bayesian_2012,
	title = {Bayesian {SEM}: {A} more ﬂexible representation of substantive theory},
	doi = {10.1037/a0026802},
	language = {en},
	author = {Muthen, Bengt and Asparouhov, Tihomir},
	year = {2012},
	pages = {78},
	file = {Muthen and Asparouhov - Bayesian SEM A more ﬂexible representation of sub.pdf:/home/michi/Zotero/storage/2YF49DGA/Muthen and Asparouhov - Bayesian SEM A more ﬂexible representation of sub.pdf:application/pdf},
}

@article{betancourt_conceptual_2018,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
	urldate = {2021-08-30},
	journal = {arXiv:1701.02434 [stat]},
	author = {Betancourt, Michael},
	month = jul,
	year = {2018},
	note = {arXiv: 1701.02434},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/home/michi/Zotero/storage/24RF9EWC/1701.html:text/html;arXiv Fulltext PDF:/home/michi/Zotero/storage/Q73W3YHC/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf},
}

@article{lu_bayesian_2016,
	title = {Bayesian {Factor} {Analysis} as a {Variable}-{Selection} {Problem}: {Alternative} {Priors} and {Consequences}},
	volume = {51},
	issn = {0027-3171, 1532-7906},
	shorttitle = {Bayesian {Factor} {Analysis} as a {Variable}-{Selection} {Problem}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00273171.2016.1168279},
	doi = {10.1080/00273171.2016.1168279},
	abstract = {Factor analysis is a popular statistical technique for multivariate data analysis. Developments in the structural equation modeling framework have enabled the use of hybrid confirmatory/exploratory approaches in which factor-loading structures can be explored relatively flexibly within a confirmatory factor analysis (CFA) framework. Recently, Muthén \& Asparouhov proposed a Bayesian structural equation modeling (BSEM) approach to explore the presence of cross loadings in CFA models. We show that the issue of determining factor-loading patterns may be formulated as a Bayesian variable selection problem in which Muthén and Asparouhov’s approach can be regarded as a BSEM approach with ridge regression prior (BSEM-RP). We propose another Bayesian approach, denoted herein as the Bayesian structural equation modeling with spike-and-slab prior (BSEM-SSP), which serves as a one-stage alternative to the BSEM-RP. We review the theoretical advantages and disadvantages of both approaches and compare their empirical performance relative to two modification indices-based approaches and exploratory factor analysis with target rotation. A teacher stress scale data set is used to demonstrate our approach.},
	language = {en},
	number = {4},
	urldate = {2021-09-07},
	journal = {Multivariate Behavioral Research},
	author = {Lu, Zhao-Hua and Chow, Sy-Miin and Loken, Eric},
	month = jul,
	year = {2016},
	pages = {519--539},
	file = {Lu et al. - 2016 - Bayesian Factor Analysis as a Variable-Selection P.pdf:/home/michi/Zotero/storage/B6H9T4QI/Lu et al. - 2016 - Bayesian Factor Analysis as a Variable-Selection P.pdf:application/pdf},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	language = {en},
	number = {482},
	urldate = {2021-09-08},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	pages = {681--686},
	file = {Park and Casella - 2008 - The Bayesian Lasso.pdf:/home/michi/Zotero/storage/63LSCEKS/Park and Casella - 2008 - The Bayesian Lasso.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	issn = {2517-6161},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1996.tb02080.x},
	doi = {10.1111/j.2517-6161.1996.tb02080.x},
	abstract = {We propose a new method for estimation in linear models. The ‘lasso’ minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1996.tb02080.x},
	keywords = {quadratic programming, regression, shrinkage, subset selection},
	pages = {267--288},
	file = {Snapshot:/home/michi/Zotero/storage/7WXZE2SW/j.2517-6161.1996.tb02080.html:text/html;Full Text PDF:/home/michi/Zotero/storage/2JLTWQ3R/Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf},
}

@article{carpenter_stan_2017,
	title = {Stan: {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	copyright = {Copyright (c) 2017 Bob Carpenter, Andrew Gelman, Matthew D. Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li, Allen Riddell},
	issn = {1548-7660},
	shorttitle = {Stan},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v076i01},
	doi = {10.18637/jss.v076.i01},
	abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
	language = {en},
	number = {1},
	urldate = {2021-09-10},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	month = jan,
	year = {2017},
	note = {Number: 1},
	keywords = {algorithmic differentiation, Bayesian inference, probabilistic programming, Stan},
	pages = {1--32},
	file = {Full Text:/home/michi/Zotero/storage/7NPIICKB/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf:application/pdf},
}

@article{merkle_efficient_2020,
	title = {Efficient {Bayesian} {Structural} {Equation} {Modeling} in {Stan}},
	url = {http://arxiv.org/abs/2008.07733},
	abstract = {Structural equation models comprise a large class of popular statistical models, including factor analysis models, certain mixed models, and extensions thereof. Model estimation is complicated by the fact that we typically have multiple interdependent response variables and multiple latent variables (which may also be called random effects or hidden variables), often leading to slow and inefficient MCMC samples. In this paper, we describe and illustrate a general, efficient approach to Bayesian SEM estimation in Stan, contrasting it with previous implementations in R package blavaan (Merkle \& Rosseel, 2018). After describing the approaches in detail, we conduct a practical comparison under multiple scenarios. The comparisons show that the new approach is clearly better. We also discuss ways that the approach may be extended to other models that are of interest to psychometricians.},
	urldate = {2021-09-10},
	journal = {arXiv:2008.07733 [stat]},
	author = {Merkle, Edgar C. and Fitzsimmons, Ellen and Uanhoro, James and Goodrich, Ben},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.07733},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/michi/Zotero/storage/XVTSVW7E/Merkle et al. - 2020 - Efficient Bayesian Structural Equation Modeling in.pdf:application/pdf;arXiv.org Snapshot:/home/michi/Zotero/storage/5MTICTCA/2008.html:text/html},
}

@article{tibshirani_regression_2011,
	title = {Regression shrinkage and selection via the lasso: a retrospective},
	volume = {73},
	issn = {1369-7412},
	shorttitle = {Regression shrinkage and selection via the lasso},
	url = {https://www.jstor.org/stable/41262671},
	abstract = {In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso.},
	number = {3},
	urldate = {2021-09-16},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Tibshirani, Robert},
	year = {2011},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {273--282},
}

@article{hsiang_bayesian_1975,
	title = {A {Bayesian} {View} on {Ridge} {Regression}},
	volume = {24},
	issn = {0039-0526},
	url = {https://www.jstor.org/stable/2987923},
	doi = {10.2307/2987923},
	number = {4},
	urldate = {2021-09-16},
	journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Hsiang, T. C.},
	year = {1975},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {267--268},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/4J2U4JX8/Hsiang - 1975 - A Bayesian View on Ridge Regression.pdf:application/pdf},
}

@article{hoerl_ridge_2000,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {42},
	issn = {0040-1706},
	shorttitle = {Ridge {Regression}},
	url = {https://www.jstor.org/stable/1271436},
	doi = {10.2307/1271436},
	abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
	number = {1},
	urldate = {2021-09-16},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {2000},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {80--86},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/N63Z3K8Z/Hoerl and Kennard - 2000 - Ridge Regression Biased Estimation for Nonorthogo.pdf:application/pdf},
}

@article{zhang_criteria_2021,
	title = {Criteria for {Parameter} {Identification} in {Bayesian} {Lasso} {Methods} for {Covariance} {Analysis}: {Comparing} {Rules} for {Thresholding}, \textit{p} -value, and {Credible} {Interval}},
	issn = {1070-5511, 1532-8007},
	shorttitle = {Criteria for {Parameter} {Identification} in {Bayesian} {Lasso} {Methods} for {Covariance} {Analysis}},
	url = {https://www.tandfonline.com/doi/full/10.1080/10705511.2021.1945456},
	doi = {10.1080/10705511.2021.1945456},
	abstract = {The lasso is a commonly used regularization method that is increasing used in structural equation models (SEMs). Under the Bayesian framework, lasso is rendered more flexible and readily produces estimates of standard errors and the penalty parameter. However, in practice, it remains unclear what decision rule is appropriate for parameter identification; in other words, determining what size estimate is large enough to be included into the model. The current study compared three decision rules for parameter identifica­ tion – thresholding, p-value, and credible interval in confirmatory factor analysis. Specifically, two distinct parameter spaces were studied: cross-loadings and residual correlations. Results showed that the thresh­ olding rule performed best in balancing power and Type I error rate. Different thresholds for standardized estimates were needed for different conditions. Guidelines for parameter identification and recom­ mended thresholding values were also provided. Results of the current study have the potential to extend to a broad range of SEMs.},
	language = {en},
	urldate = {2021-09-16},
	journal = {Structural Equation Modeling: A Multidisciplinary Journal},
	author = {Zhang, Lijin and Pan, Junhao and Ip, Edward Haksing},
	month = aug,
	year = {2021},
	pages = {1--10},
	file = {Zhang et al. - 2021 - Criteria for Parameter Identification in Bayesian .pdf:/home/michi/Zotero/storage/FYK98DKJ/Zhang et al. - 2021 - Criteria for Parameter Identification in Bayesian .pdf:application/pdf},
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/25734098},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	number = {2},
	urldate = {2021-09-16},
	journal = {Biometrika},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	year = {2010},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {465--480},
	file = {JSTOR Full Text PDF:/home/michi/Zotero/storage/2VUZWZNY/CARVALHO et al. - 2010 - The horseshoe estimator for sparse signals.pdf:application/pdf},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	doi = {10.1214/17-EJS1337SI},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	pages = {5018--5051},
	file = {Full Text:/home/michi/Zotero/storage/DMBT7QEE/Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf:application/pdf;Snapshot:/home/michi/Zotero/storage/9THVXYJ3/17-EJS1337SI.html:text/html},
}

@book{schoot_small_2020,
	address = {London New York},
	series = {European {Association} of {Methodology} series},
	title = {Small sample size solutions: a guide for applied researchers and practitioners\$fedited by {Rens} van de {Schoot} and {Milica} {Miočević}},
	isbn = {978-0-367-22189-8 978-0-367-22222-2},
	shorttitle = {Small sample size solutions},
	language = {en},
	publisher = {Routledge},
	editor = {Schoot, Rens van de and Miočević, Milica},
	year = {2020},
	file = {Schoot and Miočević - 2020 - Small sample size solutions a guide for applied r.pdf:/home/michi/Zotero/storage/H24PDG4V/Schoot and Miočević - 2020 - Small sample size solutions a guide for applied r.pdf:application/pdf},
}
