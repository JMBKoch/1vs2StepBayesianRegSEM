---
title             : "Getting a Step Ahead: Using the Regularized Horseshoe Prior to Regularize Cross-Loadings in Bayesian CFA"
shorttitle        : "Getting a Step Ahead: Using the Regularized Horseshoe Prior to Regularize Cross-Loadings in Bayesian CFA"
wordcount         : "X"
bibliography      : ["refs.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
documentclass     : "apa6"
output            : 
  papaja::apa6_pdf:
    includes:
      in_header: "preamble.tex"
    latex_engine: xelatex
editor_options: 
  markdown: 
    wrap: 72
---

```{=tex}
% move text to bottom of page
\vfill
Research Proposal \\
Michael Koch (6412157)\\
Methodology and Statistics for the Behavioral, Biomedical, and Social Sciences \\
Supervisor: Dr. Sara van Erp \\ 
Email: j.m.b.koch@students.uu.nl \\
Word Count: 746 \\
Intented Journal of Publication: Structural Equation Modeling \\
FETC permission obtained on 27'th September 2021

% make page numbers start from second page 
\pagenumbering{arabic}
\setcounter{page}{0}
\thispagestyle{empty}
% make page numbers from second page 
\pagestyle{plain}
```
```{r setup, include = FALSE}
library("papaja")
library(LaplacesDemon) # for horseshoe density 
library(ggplot2)
library(magrittr)
library(jtools) # for apa ggplot theme
library(latex2exp)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(0704)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\clearpage

*Confirmatory Factor Analysis (CFA)* is an essential tool for modeling
measurement structures. Usually, all cross-loadings, factor
loadings that relate items to factors that they theoretically do not belong to, are fixed to zero to identify the model. This often leads to poor model fit,
and forces researchers to free some cross-loadings again
to improve fit, a practice that is flawed for a variety of reasons,
among which risking capitalization on chance
[@muthen_bayesian_2012]. As solution,
@muthen_bayesian_2012 proposed that rather than fixing *all*
cross-loadings to zero, one should should assume that *most*
cross-loadings are zero. Formally, this is achieved by setting the
so-called *Small Variance Normal Prior* for the cross-loadings, which is a
normal distribution with mean zero and a very small variance (e.g.:
$\sigma^2 = 0.01, \  \sigma^2 = 0.001$). This prior has a large peak at zero, and very thin tails (see Figure 1). Hence, it attaches large prior mass to cross-loadings of or near zero, while attaching almost no prior mass to cross-loadings further from zero. Consequently, all cross-loadings in the model are
shrunken. The larger the prior's variance, the more
admissive the model is in the amount of deviation from zero it allows.

An issue with @muthen_bayesian_2012's approach is that not only the cross-loadings
close to zero that are considered irrelevant are shrunken to zero, as
desired, but also the ones further from zero are shrunken
towards zero, which introduces bias. The method thus requires a two-step
approach. First, the model is estimated with shrinkage. Then the model
is re-estimated, with cross-loadings that have been shrunken to zero in
the previous step fixed to zero, and the remaining cross-loadings
estimated without shrinkage. This process is tedious and computationally expensive. Therefore, alternative priors need
to be identified that can outperform the Small Variance Normal Prior *in a single step*.

@lu_bayesian_2016 pointed out that the approach by
@muthen_bayesian_2012 can be viewed as a form of *Regularization*, where cross-loadings that are relevant in modeling a factor structure are selected. The literature on regularization in regression
[see @van_erp_shrinkage_2019 for an overview] can thus provide
alternative priors. The *Regularized Horseshoe Prior*
[@piironen_sparsity_2017], appears as particularly promising. This prior still has a sharp peak at zero resulting in shrinkage to zero of small coefficients (see Figure 1). However, it has much heavier tails, resulting in practically no shrinkage of larger coefficients. This prior is thus the best of both worlds, by only shrinking irrelevant parameters to zero, within one step [@piironen_sparsity_2017].


```{r dev='cairo_pdf', include=T, echo=F, warning=F, fig.cap="Density Plots of the Regularization Priors of Interest"}
# Make Figure 1 with 
ndraws <- 5e+05 # 10000 draws
# sample Small Variance Normal Prior
smallVar <- rnorm(ndraws, mean = 0, sd = 0.1)

# sample regularized horseshoe prior
regHs <- rep(NA, ndraws)
for(i in 1:ndraws){
  c2 <- rinvgamma(1, shape=0.5, scale=1)
  lambda <- rhalfcauchy(1, scale=1)
  tau <- rhalfcauchy(1, scale=1)
  lambda2_tilde <- c2 * lambda^2/(c2 + tau^2*lambda^2)
  regHs[i] <- rnorm(1, 0, sqrt(tau^2*lambda2_tilde))
}

# make plot
plotDat <- 
data.frame(dens = c(smallVar, regHs), 
          prior = as.factor(rep(c("Small Variance Normal Prior", "Regularized Horseshoe Prior"), each = ndraws)),
          asymp = rep(0, ndraws)) 

levels(plotDat$prior) <- c("Regularized Horseshoe Prior", "Small Variance Normal Prior (\u03c3\u00B2 = 0.01)") 

plotDat %>% 
  ggplot(aes(x = dens, fill = prior, linetype = prior)) + 
  geom_density(alpha = .5)+
  geom_vline(aes(xintercept = asymp), linetype = "dashed") +
  xlim(-.5, .5)+
  labs(x = "Size Cross-Loading", title = NULL)+
  theme_apa(legend.pos = "bottom")
```

While the Regularized Horseshoe Prior has been shown to perform well in
the selection of relevant predictors in regression
[@piironen_sparsity_2017; @van_erp_shrinkage_2019], no previous research
has validated its performance in selecting relevant cross-loadings in
CFA. To fill this gap, the aim of this study is to compare the
Regularized Horseshoe Prior to the Small Variance Normal Prior in their
performance in selecting the true factor structure in CFA.

## Analytic Strategy

A Monte Carlo simulation study is conducted using stan
[@stan_development_team_stan_2021]. As main outcomes, we consider the bias in the estimated cross-loadings, and the trade-off in true positives vs. false positives in correctly identifying non-zero cross-loadings as non-zero (ROC curves). Hereby, the main criterion in selecting cross-loadings as non-zero is whether their 95% HPD interval contains zero [@zhang_criteria_2021]. 

We include two factor structures, one with a single, and one with three non-zero cross-loadings. Each model consists of three factors a three items. Factors are scaled by setting their means to zero and their variances to one. The correlations between all factors is set to 0.5, and the residual variance of all items to 0.3. We vary between three sample sizes (100, 200, 300), and three magnitudes of the cross-loadings (0.1, 0.2, 0.3). 

Following @muthen_bayesian_2012 we consider three values for the hyperparameter of the Small Variance Normal Prior ($\sigma^2$ = 0.001, 0.01, 0.1). The Regularized Horseshoe Prior has five hyperparameters, and varying all of them broadly would lead to an unfeasible number of conditions. We will therefore conduct a pilot study on a single simulated dataset to identify combinations of hyperparameters that are worth to consider in the main study. 
    
All models will be sampled using the No-U-Turn-Sampler [@homan_no-u-turn_2014;
@betancourt_conceptual_2018]. We aim to identify a suitable chain length in the pilot study.

\clearpage

```{tex}
\end{itemize}
```

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup
