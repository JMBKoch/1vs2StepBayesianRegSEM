---
title             : "One- vs. Two-Step Approach in Regularized Bayesian Confirmatory Factor Analysis (CFA)"
shorttitle        : "One- vs. Two-Step Approach in Regularized Bayesian CFA"

wordcount         : "X"

bibliography      : ["refs.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
output            : 
  papaja::apa6_pdf:
    includes:
      in_header: "preamble.tex"
---

```{=tex}
% move text to bottom of page
\vfill
Research Proposal \\
Michael Koch (6412157)\\
Methodology and Statistics for the Behavioral, Biomedical, and Social Sciences \\
Supervisor: Dr. Sara van Erp \\ 
Email: j.m.b.koch@students.uu.nl \\
Word Count: 746 \\
Intented Journal of Publication: Structural Equation Modeling \\
FETC permission obtained on 27'th September 2021

% make page numbers start from second page 
\pagenumbering{arabic}
\setcounter{page}{0}
\thispagestyle{empty}
% make page numbers from second page 
\pagestyle{plain}
```

```{r setup, include = FALSE}
library("papaja")
library(LaplacesDemon) # for horseshoe density 
library(ggplot2)
library(magrittr)
library(jtools) # for apa ggplot theme
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(0704)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\clearpage

A common model-constraint in *Confirmatory Factor Analysis (CFA)* is fixing all cross-loadings to zero. This is done both to identify the model, and to achieve an easy-to-interpret factor structure. However, this practice often leads to poor model fit. Consequently, researchers commonly free some cross-loadings based on modification indices to improve model fit. This practice, however, is flawed for a variety of reasons, among which risking capitalization on chance [@jacobucci_regularized_2016; @muthen_bayesian_2012].

As solution, @muthen_bayesian_2012 proposed that rather than fixing all cross-loadings to zero, one should should assume that *most* cross-loadings are zero. Formally, this is achieved by setting the so called *Small Variance Prior* for the cross-loadings, which is a normal distribution with mean zero and a very small variance (e.g.: $\sigma = 0.01, \  \sigma = 0.001$). Such prior gives cross-loadings of zero large prior density, but cross-loadings that are far from zero almost zero prior density  (see Figure 1 for an example with $\sigma = 0.01$). Consequently, all cross-loadings in the model are shrunken substantially, those close to zero to zero, and those far from zero towards zero. The prior's variance is thus the deciding factor in how admissive the model is of cross-loadings closer to zero. Hence, by tuning the variance of the prior, one is able to fit a systematically and continuously varying sequence of models allowing for a much more continuous process of model identification and model selection than classical CFA. 

However, an issue with this method is that not only the cross-loadings close to zero that are considered irrelevant are shrunken to zero, as desired. Also the cross-loadings that are further from zero, are shrunken towards zero, which introduces bias. The method thus requires a two-step approach. First, the model is estimated with shrinkage. Then the model is re-estimated where the cross-loadings that have been shrunken to zero in the previous step are fixed to zero, and the remaining cross-loadings are estimated without shrinkage. To overcome this, alternative regularization priors need to be identified that can outperform the small-variance prior *in a single step*. 

@lu_bayesian_2016 pointed out that the Bayesian CFA approach by @muthen_bayesian_2012 can be viewed as a form of *Regularization*, where rather than selecting variables that are relevant as predictors in a regression model, cross-loadings that are relevant in modeling a factor structure are selected. The literature on regularization in a regression context [see @van_erp_shrinkage_2019 for an overview] can thus provide alternative priors. Here, the *Regularized Horseshoe Prior* [@piironen_sparsity_2017], an extension of the *Horseshoe prior* [@carvalho_horseshoe_2010], appears as particularly promising. Both priors are characterized by a *global* shrinkage component $\omega$, shrinking all parameters towards zero (explaining the steep peak $\lambda_c = 0$ in Figure 1), and a  *local* component $\tau_j$, which gives the prior its heavy tails and thereby allows large parameters to escape the shrinkage entirely^[We deviate from the common notation of the global shrinkage parameter as $\lambda$, as that letter is commonly reserved for factor loadings in CFA.].  Although being one of the core qualities of the original Horseshoe Prior, in practice not shrinking large coefficients at all can lead to computational issues in the MCMC sampling of the model [@ghosh_use_2018; @piironen_sparsity_2017]. To avoid this, the *Regularized* Horseshoe Prior^[Following previous research, we set the hyper-parameters  $\nu = 4$ and $s^2 = 2$ [@piironen_sparsity_2017; @van_erp_shrinkage_2019].] is designed to ensure that there is always a little shrinkage, even for large parameters [@piironen_sparsity_2017]:


$$\lambda_c | \hat{\tau}, \omega \sim \mathcal{N}(0, \ \hat{\tau}^2 \omega), \ with \ \hat{\tau}^2 = \frac{c^2\tau_j^2}{c^2 + \omega^2 \tau_j^2}$$
$$\omega | \omega_0^2 \sim \mathcal{N}(0,\  \omega_0^2), \ with \  \omega_0 = \frac{p_0}{p-p_0}\frac{\sigma}{\sqrt{N}}$$
$$\tau_j \sim \mathcal{C^+}(0, \ 1)$$
$$c^2 | \nu, s^2 \sim \mathcal{IG}(\nu/2, \  \nu s^2/2),$$
where $p_0$ represents a prior guess of the number of relevant cross-loadings. 

```{r, echo=F, warning=F, fig.cap="Density Plots of the Regularization Priors of Interest"}
# Make Figure 1 with 
ndraws <- 5e+05 # 10000 draws
# sample small variance prior
smallVar <- rnorm(ndraws, mean = 0, sd = 0.1)

# sample regularized horseshoe prior
regHs <- rep(NA, ndraws)
for(i in 1:ndraws){
  c2 <- rinvgamma(1, shape=0.5, scale=1)
  lambda <- rhalfcauchy(1, scale=1)
  tau <- rhalfcauchy(1, scale=1)
  lambda2_tilde <- c2 * lambda^2/(c2 + tau^2*lambda^2)
  regHs[i] <- rnorm(1, 0, sqrt(tau^2*lambda2_tilde))
}

# make plot
data.frame(dens = c(smallVar, regHs), 
          prior = rep(c("Small Variance Prior", "Regularized Horseshoe Prior"), each = ndraws),
          asymp = rep(0, ndraws)) %>% 
  ggplot(aes(x = dens, fill = prior, linetype = prior)) + 
  geom_density(alpha = .5)+
  geom_vline(aes(xintercept = asymp), linetype = "dashed") +
  xlim(-.3, .3)+
  labs(x = "Size Cross-Loading", title = NULL)+
  theme_apa(legend.pos = "bottom")
```

While the Regularized Horseshoe Prior has been shown to perform well in the selection of relevant predictors in regression [@piironen_sparsity_2017; @van_erp_shrinkage_2019], no previous research has validated its performance in the context of selecting relevant cross-loadings in CFA. To fill this gap, the aim of this study is to compare the Regularized Horseshoe Prior to the Small Variance Prior in their performance in selecting the true factor structure in Confirmatory Factor Analysis (CFA). 


## Analytic Strategy

A Monte Carlo simulation study is conducted using stan [@carpenter_stan_2017]. 
True Positives vs. False Positives in estimating truly non-zero cross-loadings as non-zero are considered as main outcome.
Conditions are based on [@lu_bayesian_2016] and will include two factor structures (1 non-zero cross-loading, several non-zero cross-loadings), three sample sizes (100, 200, 300), and three magnitudes of the cross-loadings (0.1, 0.2, 0.3), yielding a total of $2 \times 3 \times 3 = 18$ main conditions. All models will be sampled using the NUTS-sampler [@betancourt_conceptual_2018], where each time four chains are sampled for 1000 iterations, following a burn-in period of 1000.

\clearpage

```{tex}
\end{itemize}
```

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
