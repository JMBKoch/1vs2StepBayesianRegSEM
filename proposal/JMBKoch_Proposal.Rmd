---
title             : "One- vs. Two-Step Approach in Regularized Bayesian Confirmatory Factor Analysis (CFA)"
shorttitle        : "One- vs. Two-Step Approach in Regularized Bayesian CFA"
wordcount         : "X"
bibliography      : ["refs.bib"]
floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
documentclass     : "apa6"
output            : 
  papaja::apa6_pdf:
    includes:
      in_header: "preamble.tex"
editor_options: 
  markdown: 
    wrap: 72
---

```{=tex}
% move text to bottom of page
\vfill
Research Proposal \\
Michael Koch (6412157)\\
Methodology and Statistics for the Behavioral, Biomedical, and Social Sciences \\
Supervisor: Dr. Sara van Erp \\ 
Email: j.m.b.koch@students.uu.nl \\
Word Count: 750 \\
Intented Journal of Publication: Structural Equation Modeling \\
FETC permission obtained on 27'th September 2021

% make page numbers start from second page 
\pagenumbering{arabic}
\setcounter{page}{0}
\thispagestyle{empty}
% make page numbers from second page 
\pagestyle{plain}
```
```{r setup, include = FALSE}
library("papaja")
library(LaplacesDemon) # for horseshoe density 
library(ggplot2)
library(magrittr)
library(jtools) # for apa ggplot theme
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(0704)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\clearpage

*Confirmatory Factor Analysis (CFA)* is an essential tool for modeling
measurement structures. Usually, all cross-loadings, factor
loadings that relate items to factors that they theoretically do not belong to, are fixed to zero to identify the model. This often leads to poor model fit,
and forces researchers to free some cross-loadings again
to improve fit, a practice that is flawed for a variety of reasons,
among which risking capitalization on chance
[@jacobucci_regularized_2016; @muthen_bayesian_2012]. As solution,
@muthen_bayesian_2012 proposed that rather than fixing *all*
cross-loadings to zero, one should should assume that *most*
cross-loadings are zero. Formally, this is achieved by setting the
so-called *Small Variance Prior* for the cross-loadings, which is a
normal distribution with mean zero and a very small variance (e.g.:
$\sigma = 0.01, \  \sigma = 0.001$). Such prior gives cross-loadings of
zero large prior density, but cross-loadings that are far from zero
almost no prior density (see Figure 1 for an example with
$\sigma = 0.01$). Consequently, all cross-loadings in the model are
shrunken. The larger the prior's variance, the more
admissive the model is in the amount of deviation from zero it allows.
Hence, by tuning the variance of the prior, one is able to fit a
systematically and continuously varying sequence of models. This allows
for a much more continuous process of model identification than classical CFA.

An issue with @muthen_bayesian_2012's approach is that not only the cross-loadings
close to zero that are considered irrelevant are shrunken to zero, as
desired, but also the ones further from zero are shrunken
towards zero, which introduces bias. The method thus requires a two-step
approach. First, the model is estimated with shrinkage. Then the model
is re-estimated, with cross-loadings that have been shrunken to zero in
the previous step fixed to zero, and the remaining cross-loadings
estimated without shrinkage. To overcome this, alternative priors need
to be identified that can outperform the small-variance prior *in a
single step*.

@lu_bayesian_2016 pointed out that the approach by
@muthen_bayesian_2012 can be viewed as a form of *Regularization*, where
rather than selecting variables that are relevant as predictors in a
regression model, cross-loadings that are relevant in modeling a factor
structure are selected. The literature on regularization in regression
[see @van_erp_shrinkage_2019 for an overview] can thus provide
alternative priors. The *Regularized Horseshoe Prior*
[@piironen_sparsity_2017], an extension of the *Horseshoe prior*
[@carvalho_horseshoe_2010], appears as particularly promising. Both
priors are characterized by a *global* shrinkage component $\omega$,
shrinking all parameters towards zero (explaining the steep peak at zero
in Figure 1), and a *local* component $\tau_j$, which
gives the prior its heavy tails and thereby allows large parameters to
escape the shrinkage entirely[^1]. Although being one of the core
qualities of the original Horseshoe Prior, in practice not shrinking
large coefficients at all leads to estimation issues [@ghosh_use_2018; @piironen_sparsity_2017]. To
avoid this, the *Regularized* Horseshoe Prior[^2] is designed to ensure
that even for large parameters there is at least a little shrinkage
[@piironen_sparsity_2017]:

[^1]: We deviate from the common notation of the global shrinkage
    parameter as $\lambda$, as that letter is commonly reserved for
    factor loadings in CFA.

[^2]: Following previous research, we set the hyper-parameters $\nu = 4$
    and $s^2 = 2$ [@piironen_sparsity_2017; @van_erp_shrinkage_2019].

$$\lambda_j | \hat{\tau}, \omega \sim \mathcal{N}(0, \ \hat{\tau}^2 \omega), \ with \ \hat{\tau}^2 = \frac{c^2\tau_j^2}{c^2 + \omega^2 \tau_j^2}$$
$$\omega | \omega_0^2 \sim \mathcal{N}(0,\  \omega_0^2), \ with \  \omega_0 = \frac{p_0}{p-p_0}\frac{\sigma}{\sqrt{N}}$$
$$\tau_j \sim \mathcal{C^+}(0, \ 1)$$
$$c^2 | \nu, s^2 \sim \mathcal{IG}(\nu/2, \  \nu s^2/2),$$ where $p_0$
represents a prior guess of the number of relevant cross-loadings.

```{r, echo=F, warning=F, fig.cap="Density Plots of the Regularization Priors of Interest"}
# Make Figure 1 with 
ndraws <- 5e+05 # 10000 draws
# sample small variance prior
smallVar <- rnorm(ndraws, mean = 0, sd = 0.1)

# sample regularized horseshoe prior
regHs <- rep(NA, ndraws)
for(i in 1:ndraws){
  c2 <- rinvgamma(1, shape=0.5, scale=1)
  lambda <- rhalfcauchy(1, scale=1)
  tau <- rhalfcauchy(1, scale=1)
  lambda2_tilde <- c2 * lambda^2/(c2 + tau^2*lambda^2)
  regHs[i] <- rnorm(1, 0, sqrt(tau^2*lambda2_tilde))
}

# make plot
data.frame(dens = c(smallVar, regHs), 
          prior = rep(c("Small Variance Prior", "Regularized Horseshoe Prior"), each = ndraws),
          asymp = rep(0, ndraws)) %>% 
  ggplot(aes(x = dens, fill = prior, linetype = prior)) + 
  geom_density(alpha = .5)+
  geom_vline(aes(xintercept = asymp), linetype = "dashed") +
  xlim(-.3, .3)+
  labs(x = "Size Cross-Loading", title = NULL)+
  theme_apa(legend.pos = "bottom")
```

While the Regularized Horseshoe Prior has been shown to perform well in
the selection of relevant predictors in regression
[@piironen_sparsity_2017; @van_erp_shrinkage_2019], no previous research
has validated its performance in selecting relevant cross-loadings in
CFA. To fill this gap, the aim of this study is to compare the
Regularized Horseshoe Prior to the Small Variance Prior in their
performance in selecting the true factor structure in Confirmatory
Factor Analysis (CFA).

## Analytic Strategy

A Monte Carlo simulation study is conducted using stan
[@stan_development_team_stan_2021]. True Positives vs. False Positives
in estimating truly non-zero cross-loadings as non-zero are considered
as main outcome. Conditions are based on @lu_bayesian_2016 and will
include two factor structures (one non-zero cross-loading, two
non-zero cross-loadings), three sample sizes (100, 200, 300), and three
magnitudes of the cross-loadings (0.1, 0.2, 0.3). All models will be sampled
using the No-U-Turn-sampler [@homan_no-u-turn_2014;
@betancourt_conceptual_2018], where each time four chains are sampled
for 1000 iterations, following a burn-in period of 1000.

\clearpage

```{tex}
\end{itemize}
```

# References

```{=tex}
\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}
```
::: {#refs custom-style="Bibliography"}
:::

\endgroup
