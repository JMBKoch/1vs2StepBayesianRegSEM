---
title             : "One- vs. Two-Step Approach in Regularized Bayesian Structural Equation Modeling (SEM)"
shorttitle        : "One- vs. Two-Step Approach in Regularized Bayesian SEM"

wordcount         : "X"

bibliography      : ["refs.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
output            : 
  papaja::apa6_pdf:
    includes:
      in_header: "preamble.tex"
---

```{=tex}
% move text to bottom of page
\vfill
Research Proposal \\
Michael Koch (6412157)\\
Methodology and Statistics for the Behavioral, Biomedical, and Social Sciences \\
Supervisor: Dr. Sara van Erp \\ 
Email: j.m.b.koch@students.uu.nl \\
Word Count: 750 \\
Intented Journal of Publication: Structural Equation Modeling

% make page numbers start from second page 
\pagenumbering{arabic}
\setcounter{page}{0}
\thispagestyle{empty}
% make page numbers from second page 
\pagestyle{plain}
```

```{r setup, include = FALSE}
library("papaja")
library(LaplacesDemon) # for horseshoe density 
library(ggplot2)
library(magrittr)
library(jtools) # for apa ggplot theme
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(0704)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

\clearpage


In the context of *Confirmatory Factor Analysis (CFA)* a popular model falling under the broader class of *Structural Equation Modeling (SEM)*, the desire for model simplicity manifest itself in the common practice of fitting models with a simple structure. Here, so-called "cross-loadings" , are fixed to zero. This serves the purpose of both identifying the model, and yielding a factor structure that can be interpreted straightforwardly. However, often times, this practice leads to poor model fit. Consequently, researchers commonly free some cross-loadings based on modification indices to improve model fit. This practice, however, is flawed for a variety of reason, among which risking capitalization on chance [REFs, there's plenty].

As solution, @muthen_bayesian_2012 proposed *Bayesian SEM*. For CFA models, they argued that rather than fixing all cross-loadings to zero, one should make a more realistic and flexible assumption. ... 

Formally, this is achieved by setting the so called Small Variance Prior for the cross-loadings, which is a normal distribution with mean zero and a very small variance $\sigma$ (e.g.: $\sigma = 0.01, \  \sigma = 0.001$)


$$\mathcal{N}(0, \sigma)$$


  By tuning the variance of the prior, one is able to fit a systematically, continuously varying sequence of models. This, in turn,  allows for a more continuous process of model identification and model selection. 

  However, a core issue with the approach by @muthen_bayesian_2012 is that applying the Small Variance Prior requires a Two-step approach [@lu_bayesian_2016]. Not only the cross-loadings close to zero that can be considered irrelevant are shrunken to zero, as desired.  Also the cross-loadings that are further from zero, and are therefore likely to play a more crucial role in accurately modeling the factor structure of the data are shrunken substantially, which introduces bias.  Therefore, more advanced priors need to be identified that can outperform the small-variance prior *in a single step*. 


A promising candidate is the *Regularized Horseshoe Prior* [, ], an extension of the Horseshoe prior [@carvalho_horseshoe_2010]. 

*Horseshoe prior* [@carvalho_horseshoe_2010] It is charactarized by a *global* shrinkage component $\omega$, shrinking all parameters towards zero, and a  *local* component $\tau_j$ allowing large parameters to escape shrinkage. This, in turn results in practically no shrinkage of large coefficients, shrinkage to zero for small coefficients, within a single step. 
- Mixture of Gaussians.

$$\lambda_j | \tau^2_j, \sim \mathcal{N}(0, \ \tau^2_j )$$
$$\tau_j \sim C^+(0, \ \omega) \ for \ j = 1, \ ..., \ p$$ 
$$\omega| \sigma \sim C^+(0, \sigma)$$
, where we denote from the common notation of the global shrinkage parameter as $\lambda$, as that letter is commony reserved for factor loadings in CFA.

For ever cross-loading $\lambda_j$in the model...

The *Regularized* Horseshoe Prior builds on the Horseshoe prior, by, which is achieve by ... 
 - Identification issues you get from (half-)cauchy priors [@ghosh_use_2018], especially with weakly identified parameters. 
     - Tail (half-)cauchy 
  - Ensure theres always a little shrinkage, by 
- 
-


$$\lambda_j | \hat{\tau}, \omega \sim \mathcal{N}(0, \ \hat{\tau}^2 \omega), \ with \ \hat{\tau}^2 = \frac{c^2t_j^2}{c^2 + \omega^2 \tau_j^2}$$
$$\omega | \omega_0^2 \sim \mathcal{N}(0,\  \omega_0^2), \ with \  \omega_0 = \frac{p_0}{p-p_0}\frac{\sigma}{\sqrt{N}}$$
$$\tau_j \sim \mathcal{C^+}(0, \ 1)$$
$$c^2 | \nu, s^2 \sim \mathcal{IG}(\nu/2, \  \nu s^2/2)$$


```{r, echo=F, warning=F, fig.cap="Density Plots of the Regularization Priors of Interest"}
# Make Figure 1 with 
ndraws <- 1e+05 # 10000 draws
# sample small variance prior
smallVar <- rnorm(ndraws, mean = 0, sd = 0.01)

# sample regularized horseshoe prior
regHs <- rep(NA, ndraws)
for(i in 1:ndraws){
  c2 <- rinvgamma(1, shape=0.5, scale=1)
  lambda <- rhalfcauchy(1, scale=1)
  tau <- rhalfcauchy(1, scale=1)
  lambda2_tilde <- c2 * lambda^2/(c2 + tau^2*lambda^2)
  regHs[i] <- rnorm(1, 0, sqrt(tau^2*lambda2_tilde))
}

# make plot
data.frame(dens = c(smallVar, regHs), 
          prior = rep(c("Small Variance Prior", "Regularized Horseshoe Prior"), each = 1e+05)) %>% 
  ggplot(aes(x = dens, fill = prior, linetype = prior)) + 
  geom_density(alpha = .5)+
  xlim(-.3, .3)+
  labs(x = "Size Cross-Loading", title = NULL)+
  theme_apa(legend.pos = "bottom")
```

In sum, the Regularized Horseshoe Prior is characterized by a unique combination of desired traits. This raises the expectation that it would outperform the Small Variance Prior in selecting the correct loading structure of a CFA model, within a single step. However, while previous research has shown promising results with the Regularized Horseshoe Prior in the selection of the correct predictor structure of Regression models [, ], no previous research has validated its performance in the selection of the factor structure in CFA. 

To fill this gap, the aim of this study is to compare the Regularized Horseshoe Prior to the Small Variance Prior in their performance in selecting the true factor structure in Confirmatory Factor Analysis (CFA). 


## Analytic Strategy

A Monte Carlo simulation study is conducted using stan [@carpenter_stan_2017]. 
True Positives vs. False Positives in estimating truly non-zero cross-loadings as non-zero are considered as main outcome (ROC).
Conditions are based on [@lu_bayesian_2016] and will include two factor structures (1 non-zero cross-loading, several non-zero cross-loadings), three sample sizes (100, 200, 300), and three magnitudes of the cross-loadings (0.1, 0.2, 0.3). These sizes are ...

\clearpage

```{tex}
\end{itemize}
```

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
