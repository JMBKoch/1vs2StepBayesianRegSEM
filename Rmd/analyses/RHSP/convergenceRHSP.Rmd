
---
title: "Analyzing the convergence of the RHSP" 
author: "Michael Koch"
date: "07-03-2022"
output: html_document
---


# Step1: Prepearations

```{r, message = F, warning = F}
# load packages outside of simulation
library(papaja)
#source required functions & parameters
source('~/1vs2StepBayesianRegSEM/R/functions.R')
source('~/1vs2StepBayesianRegSEM/R/parameters.R')
```

```{r, cache = T}
# load results
resultsRHSP <- read.csv("~/OneDrive/ms/thesis/output/resultsRHSP.csv", 
                        sep = " ", 
                        header = TRUE)
convRHSP <- read.csv("~/OneDrive/ms/thesis/output/convRHSP.csv",
                     sep = " ",
                     header = TRUE)
```

```{r}
# inspect a bit
nrow(resultsRHSP)
```

```{r}
(nrow(condPop)*nrow(condRHSP)*nIter) - nrow(resultsRHSP)
```

122 chains failed, which is in correspondence with the error R gave, when running the simulation on the server. Let 's find out the positions of those that failed. Perhaps they can be re-run. If they cannot, this may also be a result. 

```{r}
comp <- 1:76800
failedChainIndex <- which((comp %in% resultsRHSP$pos) == FALSE)
length(failedChainIndex)
failedChainIndex
```

These indicices are all from the first 200 iterations. We can simply find the set of conditions belong to this by subsetting the first row of the results object.

```{r}
resultsRHSP %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, iteration) %>%
  head(1)
```

Thus, with N = 100, cross = 0.2 and the hyper-parameters all on their lowest value (0.1 for scales, and 1 for df's) the chains don't converge in the majority of the cases.

These are 156 rows. Interestingly, the chains that did not fail show good convergence, which is sort of puzzling, but maybe not?

I will however, remove them from the analysis, as there are too few iterations for a reliable picture of this set of conditions (FF met Sara bespreken dit). 

```{r}
resultsRHSP <- 
  resultsRHSP %>% 
  filter(pos > 200)
```

```{r}
convRHSP <- 
  convRHSP %>% 
  filter(pos > 200)
```


# Convergence

Let's explore the convergence a bit (of the remaining set of conditions).

## Divergent transitions

```{r}
totalSumDiv <- sum(convRHSP$sumDiv)/2
totalSumDiv
totalSumDiv / (2000*nrow(convRHSP)/2)
```

In total, there are a gazillion divergent transition, but only a fraction of all transitions were divergent. Let's start by computing the percentage per individual replication. And then we can check how big that is on average per set of conditions.

```{r}
meanPropDiv <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  mutate(propDiv = sumDiv/ samplePars$nSampling) %>% 
  group_by(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross) %>%
  summarise(meanPropDiv = mean(propDiv))
meanPropDiv
```

On average, they appear to be quite low. There appears to be no set of conditions where the proportion of divergent transition is lager than 0.09. 

Let's find the indices of the runs where the divergent transitions exceeded 5%.

```{r}
indicesDIVHigher5Perc <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  mutate(propDIV = sumDiv/ samplePars$nSampling) %>% 
  filter(propDIV >= convCriteria$strict$maxPropDIV) %>% 
  select(pos) %>% 
  pull()
length(indicesDIVHigher5Perc)
```

This would remove 4474 rows.

## Effective Sample Size

Let's find the indices of all transitions where the effective sample size is less than 10% of the chain-length.

```{r}
indicesNEffTooSmall <- 
convRHSP %>% 
  filter(parameter == "n_eff") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  mutate_at(.vars = vars(-(scaleGlobal:pos)), .funs = function(x){x/samplePars$nSampling}) %>% 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x < .10) %>% 
  select(pos) %>% 
  pull()
```

```{r}
length(indicesNEffTooSmall)
```

By this you would lose 542 rows, which does not seem so bad.

### R-Hat

Let's find the indices of all transitions where the R-hat is smaller than 1.05.

```{r}
indicesRHatTooSmall <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  # get indices of all iterations where for any parameter 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x >= convCriteria$strict$maxRhat) %>% 
  select(pos) %>% 
  pull()
```


```{r}
length(indicesRHatTooSmall)
```

This technically removes 15 rows...

```{r}
indicesCombined <- unique(c(indicesRHatTooSmall, indicesNEffTooSmall
                            #, indicesDIVTooHigh
                            ))
length(indicesCombined)
```

.. but they were all part of the rows with too little n_samp. 

It's interesting to check, where we lost the rows specifically (under what conditions).

```{r}
resultsRHSP %>% 
  filter(pos %in% indicesCombined) %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos)
```

# Post-process

Here, I can subset the data based on the indices accquired above.
Next, I arrange the data based on conditions and iterations and recaculate the bias of the cross-loadings, which went wrong. 


```{r}
resultsRHSPFiltered <- 
  resultsRHSP %>% 
  filter(!(pos%in% indicesCombined)) %>% 
  arrange(scaleGlobal, scaleLocal, dfGlobal, dfLocal, nu, scaleSlab, N, cross, iteration) %>%  
  mutate(biasCrossMean_1 =abs(crossEstMean_1 - cross),
         biasCrossMean_2 = abs(crossEstMean_2 - 0),
         biasCrossMean_3 = abs(crossEstMean_3 - 0),
         biasCrossMean_4 = abs(crossEstMean_4 - 0),
         biasCrossMean_5 = abs(crossEstMean_5 - 0),
         biasCrossMean_6 = abs(crossEstMean_6 - cross))
nrow(resultsRHSPFiltered) + length(indicesCombined)
# Save filtered data as .Rds file for further analysis
saveRDS(resultsRHSPFiltered, file = "~/1vs2StepBayesianRegSEM/output/resultsRHSPFilter.Rds")
```

```{r}
nrow(resultsRHSPFiltered)
```

