---
title: "Getting A Step Ahead: Using the Regularized Horseshoe Prior (RHSP) to Select Cross-Loadings in Bayesian Regularized SEM"
#subtitle: "Research Report Presentation"
author: "Koch, J.M.B. (Michael)"
date: "3/26/2022"
output: ioslides_presentation
bibliography      : ["refs.bib"]
---

```{r, message=F, warning=F, error=F, echo=F}
library(LaplacesDemon) # for horseshoe density 
library(tidyverse)
library(jtools)
```

## The Art of Statistical Modeling

\ 

\ 

<center>

![](~/1vs2StepBayesianRegSEM/Rmd/figures/datagenVsModelMeme.png)

</center>

## Fundamental Trade-off in Statistical Modeling 

<center>

![](~/1vs2StepBayesianRegSEM/Rmd/figures/fundamentalTradeoff.png)

</center>


## Simple Structure


<center>


![](~/1vs2StepBayesianRegSEM/Rmd/figures/simpleStructure.png)


</center>


## Simple Structure


<center>
 

![](~/1vs2StepBayesianRegSEM/Rmd/figures/simpleStructureMeme.png)


</center>


## Small Variance Normal Prior (SVNP)

  + Don't just assume that cross-loadings are zero ( $\mathcal{N}(0, 0)$ )
  + Instead make a more realistic and flexible assumption [@muthen_bayesian_2012]:
    - Small Variance Prior, e.g.: $\mathcal{N}(0, 0.01)$ 
    - allowing for a more flexible model identification and model selection

## Small Variance

Issue: 

- Also (relevant) cross-loadings far from zero are shrunken [@lu_bayesian_2016] 

- Consequently other parameters in the model (e.g. the factor-correlation, main-loadings) end up substantially biased too

- As solution, a two-step procedure is required, which is cumbersome and introduces a number of researchers degrees of freedom
  
*&#8594;* More advanced priors need to be identified that can outperform the small-variance prior *in a single step*

## The Regularized Horseshoe Prior

- Extension of *Horseshoe Prior* [@carvalho_horseshoe_2010]
- Main idea: global shrinkage parameter (shrinking all cross-loadings to zero) and local shrinkage parameter (allowing the relevant cross-loadings to escape the shrinkage) [@piironen_sparsity_2017]
- Regularized adds a slab, that shrinks ALL cross-loadings at least a little bit [@piironen_sparsity_2017], to avoid identification issues [see @ghosh_use_2018]

## The Regularized Horseshoe Prior

For every Cross-loading  of factor j on item k:
$$\lambda_{jk} | \tilde{\omega}_{jk}, \tau, c\sim \mathcal{N}(0, \ \tilde{\omega}^2_{jk} \tau^2), \ with \ \tilde{\omega}^2_{jk} = \frac{c^2\omega_{jk}^2}{c^2 + \tau^2 \omega_{jk}^2},$$
$$\tau | s_{global}^2 \sim half-t_{df_{global}}(0,\  s_{global}^2), \ with \  s_{global} = \frac{p_0}{p-p_0}\frac{\sigma}{\sqrt{N}},$$
$$\omega_{jk} \sim half-t_{df_{local}}(0, \ s_{local}^2),$$
$$c^2 | df_{slab}, s_{slab} \sim \mathcal{IG}(\frac{df_{slab}}{2}, \  df_{slab} \times \frac{s_{slab}}{2}^2),$$
where $p_0$ represents a prior guess of the number of relevant cross-loadings.

## The Regularized Horseshoe Prior

```{r, include=T, echo=F, warning=F}
# Make Figure 1 with 
ndraws <- 5e+05 # 30000 draws
# sample Small Variance Normal Prior
smallVar <- rnorm(ndraws, mean = 0, sd = sqrt(0.01))

# sample regularized horseshoe prior
regHs <- rep(NA, ndraws)
for(i in 1:ndraws){
  c2 <- rinvgamma(1, shape=0.5, scale=1)
  lambda <- rhalfcauchy(1, scale=1)
  tau <- rhalfcauchy(1, scale=1)
  lambda2_tilde <- c2 * lambda^2/(c2 + tau^2*lambda^2)
  regHs[i] <- rnorm(1, 0, sqrt(tau^2*lambda2_tilde))
}

# make plot
data.frame(dens = c(smallVar, regHs), 
          prior = as.factor(rep(c("Small Variance Normal Prior (\u03c3\u00B2 = 0.01)", "Regularized Horseshoe Prior"), each = ndraws)),
          asymp = rep(0, ndraws)) %>% 
  ggplot(aes(x = dens, fill = prior, linetype = prior)) + 
  geom_density(alpha = .5)+
  geom_vline(aes(xintercept = asymp), linetype = "dashed") +
  xlim(-.5, .5)+
  labs(x = "Size Cross-Loading", title = NULL)+
  theme_apa(legend.pos = "bottom")
```

## The present study

<center>

![](~/1vs2StepBayesianRegSEM/Rmd/figures/model.png)

</center>

## The present study

- Monte Carlo Simulation using `stan` [@stan_development_team_stan_2021]
- Population conditions:
  - N: 100, 200
  - magnitude cross-loading: 0.2, 0.5
- Prior conditions:
  - $\sigma^2$: 0.1, 0.01, 0.001 [@muthen_bayesian_2012]
- Main outcomes: (mean absolute) bias, MSE, Power, Type I error rate [based on a variety of selection rules, @zhang_criteria_2021]

## Results

![](~/1vs2StepBayesianRegSEM/Rmd/figures/allPars.png){length=500px}

## Preliminary conclusion: 
  - The SVNP does pretty well with small cross-loadings 
  - But especially with larger cross-loadings, the bias becomes substantial 
  - Hereby even structural parameters are affected
  - A sensible choice of $\sigma^2$ can help to counter this

## Foreshadowing Thesis

- same population conditions as above
- RHSP hyper-parameter conditions:
  - scale of global shrinkage parameter: 0.1, 1
  - df of global shrinkage parameter: 1, 3
  - scale of local shrinkage parameter: 0.1, 1
  - df of local shrinkage parameter: 1, 3
  - scale of slab: 0.1, 1, 5
  - df of slab: 1, 3
- Worse performance in terms of convergence expected
- Better performance in terms of bias expected 

## Check out my work on Github

[Feel free to check out, criticize and improve my work!](https://www.github.com/JMBKoch/1vs2StepBayesianRegSEM)

<center>

![](~/1vs2StepBayesianRegSEM/Rmd/figures/dodge.png)

</center>

<style>
slides > slide { overflow: scroll; }
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

## References
