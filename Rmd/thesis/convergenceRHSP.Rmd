
---
title: "Analyzing the convergence of the RHSP" 
author: "Michael Koch"
date: "07-03-2022"
output: html_document
---


# Step1: Prepearations

```{r, message = F, warning = F}
# load packages outside of simulation
library(papaja)
#source required functions & parameters
source('~/1vs2StepBayesianRegSEM/R/functions.R')
source('~/1vs2StepBayesianRegSEM/R/parameters.R')
```

```{r, cache = T}
# load results
resultsRHSP <- read.csv("~/OneDrive/ms/thesis/output/resultsRHSP.csv", 
                        sep = " ", 
                        header = TRUE)
convRHSP <- read.csv("~/OneDrive/ms/thesis/output/convRHSP.csv",
                     sep = " ",
                     header = TRUE)
```

# Post-process

```{r}
resultsRHSP <- 
  resultsRHSP %>% 
  arrange(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, iteration) %>% 
  mutate(biasCrossMean_1 = abs(crossEstMean_1 - cross),
         biasCrossMean_2 = abs(crossEstMean_2 - 0),
         biasCrossMean_3 = abs(crossEstMean_3 - 0),
         biasCrossMean_4 = abs(crossEstMean_4 - 0),
         biasCrossMean_5 = abs(crossEstMean_5 - 0),
         biasCrossMean_6 = abs(crossEstMean_6 - cross)
) 
  
```

```{r}
# inspect a bit
nrow(resultsRHSP)
```

```{r}
(nrow(condPop)*nrow(condRHSP)*nIter) - nrow(resultsRHSP)
```

122 chains failed, which is in correspondence with the error R gave, when running the simulation on the server. Let 's find out the positions of those that failed. Perhaps they can be re-run. If they cannot, this may also be a result. 

```{r}
comp <- 1:76800
failedChainIndex <- which((comp %in% resultsRHSP$pos) == FALSE)
length(failedChainIndex)
failedChainIndex
```

These indicices are all from the first 200 iterations. We can simply find the set of conditions belong to thi sby subsetting the first row of results

```{r}
resultsRHSP %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, iteration) %>%
  head(1)
```


Thus, with N = 100, cross = 0.2 and the hyper-parameters all on their lowest vaslue (0.1 for scales, and 1 for df's) the chains don't converge in the majority of the cases.

Let's check the convergence diagnostics for those few replications under these conditions that did yield any output at all. We can assume that the convergence diagnostics do not not look good here

```{r}
convRHSP %>% 
  filter(pos <= 200) %>% 
  arrange(pos)
```

These are 156 rows, hence the 200-122 = 78 (times too because of the output format) chains under the first set of conditions that did not fail. Interestingly, the chains that did not fail show good convergence, which is sort of puzzling, but maybe not?

I will however, remove them from the analysis, as there are too few iterations for a reliable picture of this set of conditions (Discuss with Sara). 

```{r}
resultsRHSP <- 
  resultsRHSP %>% 
  filter(pos > 200)
```

```{r}
convRHSP <- 
  convRHSP %>% 
  filter(pos > 200)
```


# Convergence

Let's explore the convergence a bit (of the remaining set of conditions).

## Divergent transitions

```{r}
totalSumDiv <- sum(convRHSP$sumDiv)
totalSumDiv
totalSumDiv / (2000*nrow(convRHSP))
```

In total, there are a gazillion divergent transition, but only a fraction of all transitions were divergent. Let's start by computing the percentage per individual replication. And then we can check how big that is on average per set of conditions.

```{r}
meanPercDiv <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% # doesn't matter which you pick here
  mutate(percDiv = sumDiv/ samplePars$nSampling) %>% 
  group_by(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross) %>%
  summarise(meanPercDiv = mean(percDiv)*100)
meanPercDiv
```

## Effective Sample Size

Let's find the indices of all transitions where the effective sample size is less than 10% of the chain-length.

```{r}
indicesNEffTooSmall <- 
convRHSP %>% 
  filter(parameter == "n_eff") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  mutate_at(.vars = vars(-(scaleGlobal:pos)), .funs = function(x){x/samplePars$nSampling}) %>% 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x < .10) %>% 
  select(pos) %>% 
  pull()
```

```{r}
length(indicesNEffTooSmall)
```

By this you would lose 542 rows, which does not seem so bad.

### R-Hat

Let's find the indices of all transitions where the R-hat is smaller than 1.05.

```{r}
indicesRHatTooSmall <- 
convRHSP %>% 
  filter(parameter == "Rhat") %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos, lambdaMainC.1.:theta.6.) %>% 
  # get indices of all iterations where for any parameter 
  filter_at(.vars = vars(lambdaMainC.1.:theta.6.), ~.x >= 1.05) %>% 
  select(pos) %>% 
  pull()
```


```{r}
length(indicesRHatTooSmall)
```

This technically removes 15 rows...

```{r}
indicesCombined <- unique(c(indicesRHatTooSmall, indicesNEffTooSmall))
length(indicesCombined)
```

.. but they were all part of the rows with too little n_samp.


It's interesting to check, where we lost the rows specifically (under what conditions).

```{r}
resultsRHSP %>% 
  filter(pos %in% indicesCombined) %>% 
  select(scaleGlobal, dfGlobal, scaleLocal, dfLocal, scaleSlab, nu, N, cross, pos)

```


# Post-process

Here, I can subset the data based on the indices accquired above.
Next, I arrange the data based on conditions and iterations and recaculate the bias of the cross-loadings, which went wrong. 


```{r}
resultsRHSPFiltered <- 
  resultsRHSP %>% 
  filter( !(pos%in% indicesCombined)) %>% 
  arrange(scaleGlobal, scaleLocal, dfGlobal, dfLocal, nu, scaleSlab, N, cross, iteration) %>%          mutate(biasCrossMean_1 =abs(crossEstMean_1 - cross),
                biasCrossMean_2 = abs(crossEstMean_2 - 0),
                biasCrossMean_3 = abs(crossEstMean_3 - 0),
                biasCrossMean_4 = abs(crossEstMean_4 - 0),
                biasCrossMean_5 = abs(crossEstMean_5 - 0),
                biasCrossMean_6 = abs(crossEstMean_6 - cross))
nrow(resultsRHSPFiltered) + length(indicesCombined)
```